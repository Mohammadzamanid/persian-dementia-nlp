# spacy_wrapper.py  (place next to dadma_wrapper.py)
from __future__ import annotations
from pathlib import Path
import spacy

# load your model once ─────────────────────────────────────────
MODEL_PATH = Path(r"C:\Users\Fatiima\project-thesis\spacy_pos_tagger_parsbertpostagger95")
_nlp = spacy.load(str(MODEL_PATH))           # requires spaCy ≥3.7

_ZWNJ = "\u200c"
CLITICS = {"شون","مون","تون","شان","مان","تان","ایم","اید","اند","ام","ات","اش","م","ت","ش"}
MI_FORMS = {"می","مى","مي","نمی","نمى","نمي"}       # NEW
LEFT_OK = ("NOUN","PROPN","PRON","ADJ","ADP","ADV")
NON_CLITIC_WORDS = {"هم", "درخت", "پشت","برگشت", "آب‌وتاب","منتها","ناراحت","یک‌خورده","نشان","کشان","وقت","دولت","ملت","رمضان","صنعت","تجارت","عدالت","سعادت","شهادت","طبیعت","شریعت","حقیقت","معرفت",
    "مصلحت","حکمت","رحمت","نعمت","برکت","وحدت","نسبت","دعوت","حکومت","قیمت","هویت","شهرت","لیوان","گلدان","نشان","میوه","میز","خانواده","تحت","میزگرد",'حکایت',
    "ثروت","قیامت","بصیرت","غیرت","حیرت","حسرت","لغت","صحت","سلامت","طراوت","صحبت","تربیت","لیوان","سیگار","کرمان","اصفهان",'رفت','پنکه','گوله','چمباتمه','ریزش',
    "محبت","فرصت","رسالت","چهارم","جام","امانت","حمایت","شکایت","روایت","دست","نباید","انواع و اقسام","انواع‌واقسام","فنجان",'نیاید','بیاید','نی‌اید','بی‌اید','شاید','باید','ایشان','ساختمان','ساختمون','اقلام','همان','داستان‌سرایی','اعلام','سمت','راست','نظافت','کریم‌خان','حوضچه','همه',
     "حیوون","حیوان","بلم","انتهاش","هرکدوم","رفتها","رفت‌ها","هرکدام","میوه‌جات","میوهجات","جیم","یواش‌یواش","یواشیواش","بعدش","برگی","دخترخانم","اهرم","سلام","آدم","کوهستان","انجام","آب‌کش","قفسه","هندوانه","ایناهاش","مردم","خدمت","مامان","قلم","علم","ظلم","رحم","ختم","رقم","حکم","ستم","فهم","قدم","قسم","کرم","اسم",
    "اینجاش","باید","جسم","رسم","مهمان","خاموش","تنها","نیست‌ها","نیستها","گردش","بیش","علیکم","هیچ‌کدوم","هیچکدوم","حیات","آرام","آینه","کاپیتالیسم","سوسیالیسم","نظم","دستفروش","آسمان","قدیم","گلاب‌پاش","دبستان","دوران","ریش‌ریش","اقسام","تمام","قایم","نجات","دنبال","خاطرات","لرزش","نت","شکلات","اعدام","داستان","جزئیات","احترام","اقدام","اهتمام","فیلم","تیم","حرم",'دم',"پنجره","کوچه","گوجه",
     "بال‌اش","باباش","تنقلات","فلش","محکم","مراسم","پیش","ورزش","حرکات","چیست","درختان","چی","داداش",'سیمان',"اممم","رفت‌ها","رفتها","می‌کنندها","میکنندها","مریم","خوش","پیش","موش","لم","ریش","آمم","دخترخانوم","چشم","آفرینش","دوش","حالات","تصمیمات","آلات","آسمون","کش","فرش","فروش","فرش‌فروش","تصمیم","لوازم","گرم","درویش","معلوم","دوم","سوم","چهارم","نوجوان","جدی","پرت","بالاش","کفش","اونجاش","آتش","دانش","بنفش","باید","کوشش","آرایش","نمایش","حرکت","فرش","شیرینی‌جات","بخش","چکش","نقاش","فروش","خروش","خواهش","تلاش",    "میوه","کوه","توهُم","شانه","خانه","پنجره","قصه","قصابه","کرخه","کوزه",
    "آرامش","نوازش","ترکش","تموم","نقش","کمان","تیروکمان","خش","مستقیم","تابش","بسم","الرحیم","لغزش","دوش","مس‌فروش","مسفروش","دوستان","روکش","مهم","سرگرم","سیم","شهرستان","کدام","هجوم","مرحوم","فروشان","ایران","شباهت","کاشان","حالات","کاهش","افزایش","جهان","گرایش","بینش","کنش","واکنش","آغوش","پرچم","حالت","گلیم","هوش","نقوش","گوش","دوردست","عدم","درست","است","هست","نیست","است.", "هست.", "نیست."
    "عطش","پوشش","آموزش","پژوهش","پرسش","گزارش","سفارش", "میز","میوه","ساعت","سفره","بایست","قسمت","ن‌شان","تلویزیون","پخش","جهت","کابینت","خانم",'سیستم',
    "حیوانات","تشکیلات","مشکلات","مشخصات","نکات","صفات","اشتباهات","امم","علایم","ایام","زمان","دیوان","باهم", "علائم","دستورات","هم","آها","نوشابه‌فروش","نوشابهفروش", "انتها",'غزلیات','کم','نردبان','اعم',}
def _merge_clitics(tokens, non_clitic_words=None):
    if non_clitic_words is None:
        non_clitic_words = set()
    merged = []
    i = 0
    while i < len(tokens):
        t = tokens[i]
        if i+1 < len(tokens):
            left = tokens[i]["tok"]
            right = tokens[i+1]["tok"]
            left_pos = tokens[i]["pos"]
            # 1) right must be an EXACT clitic token
            if right in CLITICS:
                # 2) left host POS must be safe
                if any(left_pos.startswith(p) for p in LEFT_OK):
                    # 3) left host should not be in the exceptions list
                    if left not in non_clitic_words:
                        # 4) avoid merging single-letter hosts like "آ"
                        if len(left) >= 2:
                            merged_tok = left + _ZWNJ + right
                            merged.append({
                                "tok": merged_tok,
                                "lemma": tokens[i]["lemma"],  # lemma handled later
                                "pos": tokens[i]["pos"],
                                "had_clitic": True            # <-- mark for lemma stage
                            })
                            i += 2
                            continue
        merged.append(t)
        i += 1
    return merged

def _merge_mi_prefix(tokens: list[dict]) -> list[dict]:           # NEW
    merged = []
    i = 0
    while i < len(tokens):
        t = tokens[i]
        if (t["tok"] in MI_FORMS) and (i+1 < len(tokens)):
            nxt = tokens[i+1]
            # attach only if the next token is a verb/aux (based on tag prefix)
            if nxt["pos"].startswith(("V","VERB","AUX")):
                # keep lemma+POS of the verb; drop the standalone 'می/نمی'
                merged_tok = {
                    "tok":   t["tok"] + _ZWNJ + nxt["tok"],
                    "lemma": nxt["lemma"],
                    "pos":   nxt["pos"],
                }
                merged.append(merged_tok)
                i += 2
                continue
        merged.append(t)
        i += 1
    return merged

def spacy_tokenise(text: str, *, merge_clitics: bool = False, merge_mi: bool = False,
                   non_clitic_words=None) -> list[dict]:
    doc = _nlp(text)
    out = [{"tok": t.text, "lemma": t.lemma_ or t.text, "pos": t.tag_} for t in doc]
    if merge_mi:
        out = _merge_mi_prefix(out)
    if merge_clitics:
        out = _merge_clitics(out, non_clitic_words=NON_CLITIC_WORDS)
    return out